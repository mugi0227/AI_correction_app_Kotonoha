import os
import re
import json
import textwrap
import google.generativeai as genai
from PIL import Image
from pathlib import Path
from typing import List, Optional, Dict

async def classify_image_quality(image_path: Path) -> Dict[str, str]:
    """
    ç­”æ¡ˆç”»åƒã®å“è³ªã‚’è©•ä¾¡ã—ã€è‡ªå‹•æ·»å‰Šã«é©ã—ã¦ã„ã‚‹ã‹ã‚’åˆ¤æ–­ã—ã¾ã™ã€‚
    æˆ»ã‚Šå€¤ã¯ {"label": "OK"|"NG", "reason": "..."}ã€‚
    """
    model = genai.GenerativeModel('gemini-2.5-pro')
    img = Image.open(image_path)
    # åˆ¤å®šã‚’ã‚„ã‚„ç”˜ãã™ã‚‹ï¼ˆãƒœãƒ¼ãƒ€ãƒ¼ã¯ OK ã«å€’ã™ï¼‰
    prompt = (
        "ã‚ãªãŸã¯ç­”æ¡ˆã®å“è³ªè©•ä¾¡ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ•°å­¦ç­”æ¡ˆã®ç”»åƒã‚’æ¬¡ã®åŸºæº–ã§è©•ä¾¡ã—ã€"
        "è‡ªå‹•æ·»å‰Šã®å¯å¦ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚ãªãŠã€æ›–æ˜§ãªå ´åˆã¯ OK ã¨ã—ã¦ãã ã•ã„ã€‚\n\n"
        "[è‡ªå‹•æ·»å‰Šå¯èƒ½ (Good=OK)]\n"
        "- æ–‡å­—ãŒæ¦‚ã­èª­ã¿ã‚„ã™ã„ï¼ˆå¤šå°‘ã®ä¹±ã‚Œãƒ»ç™–ãƒ»è–„ã•ã¯è¨±å®¹ï¼‰ã€‚\n"
        "- ç­†åœ§ã‚„ç·šã®é®®æ˜ã•ãŒååˆ†ã€ã¾ãŸã¯ã‚„ã‚„è–„ã„ãŒåˆ¤èª­å¯èƒ½ã€‚\n"
        "- ã‚¹ã‚­ãƒ£ãƒ³çŠ¶æ…‹ãŒæ¦‚ã­è‰¯å¥½ã€‚è»½å¾®ãªå‚¾ããƒ»å½±ãƒ»ãƒã‚¤ã‚ºãƒ»è§£åƒåº¦ä¸è¶³ã¯ OKã€‚\n\n"
        "[æ‰‹å‹•ç¢ºèªæ¨å¥¨ (Bad=NG)]\n"
        "- å…¨ä½“çš„ã«æ¥µç«¯ã«è–„ã„/ã‹ã™ã‚Œã¦ã„ã¦åˆ¤èª­ãŒå›°é›£ã€‚\n"
        "- æ–‡å­—ã®åˆ¤èª­ãŒå›°é›£ãªãƒ¬ãƒ™ãƒ«ã§ä¹±é›‘/ãƒ–ãƒ¬ãŒé¡•è‘—ã€‚\n"
        "- å½±ãƒ»ãƒã‚¤ã‚ºãƒ»å‚¾ããŒç”šå¤§ã§ã€é‡è¦éƒ¨åˆ†ã®åˆ¤èª­ãŒé›£ã—ã„ã€‚\n\n"
        "åˆ¤å®šãƒ«ãƒ¼ãƒ«ï¼ˆç”˜ã‚ï¼‰:\n"
        "- è¿·ã†å ´åˆã‚„éƒ¨åˆ†çš„ã«æ‚ªã„ç¨‹åº¦ãªã‚‰ 'OK'ã€‚\n"
        "- 'NG' ã¯ä¸Šè¨˜Badã«æ˜ç¢ºã«è©²å½“ã™ã‚‹å ´åˆã¨ç™½ç´™ç­”æ¡ˆã®ã¿ã€‚\n\n"
        "JSONã®ã¿ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆæ—¥æœ¬èªï¼‰ã€‚\n"
        "å½¢å¼: {\"label\": \"OK|NG\", \"reason\": \"çŸ­ã„èª¬æ˜\"}"
    )
    response = await model.generate_content_async([prompt, img])
    raw = (getattr(response, 'text', '') or '').strip()
    # JSONæŠ½å‡ºï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯/ç´ ã®JSONä¸¡å¯¾å¿œï¼‰
    m = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", raw, re.IGNORECASE)
    if m:
        raw = m.group(1)
    try:
        data = json.loads(raw)
    except Exception:
        # ã†ã¾ãJSONã«ãªã‚‰ãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        up = raw.upper()
        label = 'OK' if 'OK' in up and 'NG' not in up else ('NG' if 'NG' in up else 'NG')
        reason = raw[:120]
        return {"label": label, "reason": reason}
    label = str(data.get('label', '')).strip().upper()
    if label not in ('OK', 'NG'):
        up = raw.upper()
        label = 'OK' if 'OK' in up and 'NG' not in up else ('NG' if 'NG' in up else 'NG')
    reason = str(data.get('reason', '')).strip() or ("åŸºæº–ã«åŸºã¥ãç·åˆåˆ¤å®š: " + label)
    return {"label": label, "reason": reason}

async def transcribe_images_to_markdown(image_paths: List[Path]) -> str:
    """
    è¤‡æ•°ã®ç­”æ¡ˆç”»åƒã‚’Markdownå½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã«æ›¸ãèµ·ã“ã—ã¾ã™ã€‚
    ç’°å¢ƒå¤‰æ•° TRANSCRIBE_MODE=dummy ã®å ´åˆã¯ãƒ€ãƒŸãƒ¼ã®Markdownã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    mode = os.getenv("TRANSCRIBE_MODE", "real").lower()
    if mode == "dummy":
        parts = []
        for i, img_path in enumerate(image_paths):
            parts.append(f"--- [ãƒšãƒ¼ã‚¸ {i+1}] ---\n\n(ãƒ€ãƒŸãƒ¼) {img_path.name} ã‚’æ›¸ãèµ·ã“ã—\n\n")
        dummy_md = "".join(parts)
        print("[transcribe_images_to_markdown] ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ‰ã§MDç”Ÿæˆ")
        return dummy_md

    model = genai.GenerativeModel('gemini-2.5-pro')
    full_transcription = ""
    for i, img_path in enumerate(image_paths):
        try:
            img = Image.open(img_path)
            prompt = (
                "ã‚ãªãŸã¯ãƒ—ãƒ­ã®æ–‡å­—èµ·ã“ã—å°‚é–€å®¶ã§ã™ã€‚"
                "æ·»ä»˜ã•ã‚ŒãŸæ‰‹æ›¸ãã®æ•°å­¦ç­”æ¡ˆã®ç”»åƒã¯ã€é–“é•ã„ã‚’å«ã‚“ã§ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã‚’ã€**å¿…ãšé–“é•ã„ã‚’ãã®ã¾ã¾ã«**Markdownå½¢å¼ã§æ­£ç¢ºã«æ›¸ãèµ·ã“ã—ã¦ãã ã•ã„ã€‚"
                "æ•°å¼ã¯TeXå½¢å¼ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚"
                "ç­”æ¡ˆã®å†…å®¹ã ã‘ã§ãªãã€ã€Œå¹´åº¦ãƒ»å¤§å­¦åãƒ»ç¨®åˆ¥ãƒ»ç¬¬nå•ã€ãªã©ã®ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã‚„è©¦é¨“ç¨®ã«é–¢ã‚ã‚‹ãƒ’ãƒ³ãƒˆã‚‚å¿…ãšæ›¸ãèµ·ã“ã—ã¦ãã ã•ã„ã€‚"
                "å¤§å­¦åä»¥å¤–ã«ã‚‚ã€ç§‘ç›®åï¼ˆä¾‹: æ•°å­¦â… Aã€ç‰©ç†ã€è‹±èªãªã©ï¼‰ã‚„è©¦é¨“ç¨®ãƒ©ãƒ™ãƒ«ï¼ˆä¾‹: å®ŸåŠ›ãƒ†ã‚¹ãƒˆã€å­¦åŠ›ãƒ†ã‚¹ãƒˆã€æ¨¡è©¦ã€å®šæœŸãƒ†ã‚¹ãƒˆãªã©ï¼‰ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€æ­£ç¢ºã«ãã®ã¾ã¾æ›¸ãèµ·ã“ã—ã€ã‚«ãƒ†ã‚´ãƒªåã‚„è¦‹å‡ºã—ã¨ã—ã¦æ®‹ã—ã¦ãã ã•ã„ã€‚"
                "ç•¥ç§°ã‚„è£…é£¾ã‚‚å«ã‚ã€åˆ¤åˆ¥å¯èƒ½ãªæ–‡å­—ãƒ»è¨˜å·ã¯çœç•¥ã›ãšè¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"
            )
            response = await model.generate_content_async([prompt, img])
            text = (getattr(response, 'text', '') or '').strip()
            if not text:
                raise RuntimeError("ç©ºã®å¿œç­”ã‚’å—ä¿¡")
            full_transcription += f"--- [ãƒšãƒ¼ã‚¸ {i+1}] ---\n\n{text}\n\n"
        except Exception as e:
            raise RuntimeError(f"ãƒšãƒ¼ã‚¸{i+1}ã®æ›¸ãèµ·ã“ã—ã«å¤±æ•—: {e}")
    return full_transcription

async def extract_answer_details(markdown_content: str) -> dict:
    """
    æ›¸ãèµ·ã“ã•ã‚ŒãŸMarkdownã‹ã‚‰å¤§å­¦åã€å¹´åº¦ã€ç§‘ç›®åã€å¤§å•ç•ªå·ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    """
    # 1) ã¾ãšè¦å‰‡ãƒ™ãƒ¼ã‚¹ã§ã®æŠ½å‡ºã‚’è©¦ã¿ã‚‹ï¼ˆAIã«é ¼ã‚‰ãšã«é ‘å¥ã«ï¼‰
    def _normalize_subject(s: str) -> Optional[str]:
        if not s:
            return None
        s = s.strip()
        # æ•°å­¦I/II -> æ•°å­¦1/2 ã«æ­£è¦åŒ–
        s = s.replace("â… ", "1").replace("â…¡", "2").replace("I", "1").replace("II", "2")
        m = re.search(r"æ•°å­¦\s*([12])", s)
        return f"æ•°å­¦{m.group(1)}" if m else None

    def _parse_details_from_text(text: str) -> Dict[str, str]:
        import unicodedata
        uni = None
        year = None
        subj = None
        q_num = None

        # å¹´åº¦: 2016å¹´åº¦ ç­‰
        m = re.search(r"(20\d{2})\s*å¹´åº¦", text)
        if m:
            year = m.group(1)

        # ç§‘ç›®: æ•°å­¦1/2 ãªã©ï¼ˆI/IIå«ã‚€ï¼‰
        subj = _normalize_subject(text)

        # å¤§å­¦å: ã€Œã€‡ã€‡å¤§å­¦ã€æœ€åˆã«å‡ºã‚‹ã‚‚ã®
        m = re.search(r"([ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z]+å¤§å­¦)", text)
        if m:
            uni = m.group(1)
        
        # å¤§å•ç•ªå·: ç¬¬2å•, ç¬¬ï¼’å•
        m = re.search(r"ç¬¬\s*([0-9ï¼-ï¼™]+)\s*å•", text)
        if m:
            # å…¨è§’ã‚’åŠè§’ã«
            q_num = str(unicodedata.normalize('NFKC', m.group(1)))

        # è¡¨è¡Œ: "2016å¹´åº¦ãƒ»åŒ—æµ·é“å¤§å­¦å…¨å­¦éƒ¨ãƒ»æ•°å­¦2ãƒ»ç¬¬1å•" å½¢å¼ã‹ã‚‰åˆ†è§£
        m = re.search(r"(20\d{2})å¹´åº¦ãƒ»([^ãƒ»\n]+å¤§å­¦)[^\n]*ãƒ»([^ãƒ»\n]*æ•°å­¦[12â… â…¡I]{1})[^\n]*ãƒ»ç¬¬\s*([0-9ï¼-ï¼™]+)\s*å•", text)
        if m:
            year = year or m.group(1)
            uni = uni or m.group(2)
            subj = subj or _normalize_subject(m.group(3))
            q_num = q_num or str(unicodedata.normalize('NFKC', m.group(4)))

        details: Dict[str, str] = {}
        if uni:
            details["university"] = uni
        if year:
            details["year"] = year
        if subj:
            details["subject"] = subj
        if q_num:
            details["question_number"] = q_num
        return details

    rule_based = _parse_details_from_text(markdown_content)
    # å¤§å•ã¯å¿…é ˆã§ã¯ãªã„ãŸã‚ã€ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã‹ã‚‰å¤–ã™
    if {"university", "year", "subject"}.issubset(rule_based.keys()):
        print(f"[extract_answer_details] ãƒ«ãƒ¼ãƒ«æŠ½å‡ºã«æˆåŠŸ: {rule_based}")
        return rule_based

    # 2) AIã«å•ã„åˆã‚ã›ï¼ˆJSONã®ã¿è¿”ã•ã›ã‚‹è¨­å®šã‚’å¼·åŒ–ï¼‰
    # äº’æ›æ€§é‡è¦–ã®ãŸã‚ generation_config ã¯ä½¿ã‚ãªã„ï¼ˆå¤ã„SDKã§ä¾‹å¤–ã®å¯èƒ½æ€§ï¼‰
    model = genai.GenerativeModel('gemini-2.5-pro')

    prompt_template = textwrap.dedent("""
        æ¬¡ã®Markdownã®å…ˆé ­éƒ¨åˆ†ã‹ã‚‰ä»¥ä¸‹ã®4é …ç›®ã‚’æŠ½å‡ºã—ã€JSONã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
        - university: å¤§å­¦åï¼ˆä¾‹: åŒ—æµ·é“å¤§å­¦ï¼‰
        - year: è¥¿æš¦4æ¡ï¼ˆä¾‹: 2016ï¼‰
        - subject: æ•°å­¦1 ã¾ãŸã¯ æ•°å­¦2ï¼ˆãƒ­ãƒ¼ãƒæ•°å­—ã¯åŠè§’æ•°å­—ã«æ­£è¦åŒ–ï¼‰
        - question_number: å¤§å•ã®ç•ªå·ï¼ˆä¾‹: \"2\"ï¼‰ã€‚è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° nullã€‚

        è¿”ç­”ã¯å³å¯†ãªJSONã®ã¿ã€‚èª¬æ˜ãƒ»ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãƒ»ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è£…é£¾ã¯å‡ºåŠ›ã—ãªã„ã“ã¨ã€‚

        ---
        {content}
        ---
    """)
    prompt = prompt_template.format(content=markdown_content)

    text_response = ""
    try:
        if os.getenv("EXTRACT_USE_AI", "true").lower() == "false":
            raise RuntimeError("AIæŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—(EXTRACT_USE_AI=false)")
        response = await model.generate_content_async(prompt)
        text_response = (getattr(response, 'text', '') or "").strip()
        print(f"AIã‹ã‚‰ã®ç”Ÿã®å¿œç­” (è©³ç´°æŠ½å‡º): {text_response}")  # ãƒ‡ãƒãƒƒã‚°
    except Exception as e:
        print(f"[extract_answer_details] AIæŠ½å‡ºã‚¹ã‚­ãƒƒãƒ—/å¤±æ•—: {e}")
        # å¾Œç¶šã§ãƒ«ãƒ¼ãƒ«æŠ½å‡ºã®éƒ¨åˆ†çµæœã‚’ä½¿ã†

    # ç´”JSONæƒ³å®šã€‚ã ã‚ãªã‚‰ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯/ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚µãƒ«ãƒ™ãƒ¼ã‚¸
    def _try_load_json(s: str) -> Optional[dict]:
        try:
            return json.loads(s)
        except Exception:
            return None

    data = _try_load_json(text_response)
    if not data:
        m = re.search(r"```json\s*(\{.*?\})\s*```", text_response, re.DOTALL)
        if m:
            data = _try_load_json(m.group(1))
    if not data:
        m = re.search(r"(\{[\s\S]*\})", text_response)
        if m:
            data = _try_load_json(m.group(1))

    if not data:
        # ãƒ«ãƒ¼ãƒ«æŠ½å‡ºã®éƒ¨åˆ†æˆåŠŸãŒã‚ã‚Œã°è¿”ã™ï¼ˆæœ€ä½é™ã®æƒ…å ±ï¼‰
        if rule_based:
            print("[extract_answer_details] AIæœªä½¿ç”¨/å¤±æ•—ã®ãŸã‚ãƒ«ãƒ¼ãƒ«æŠ½å‡ºã®éƒ¨åˆ†çµæœã‚’è¿”å´", rule_based)
            return rule_based
        raise ValueError("No JSON found in AI response and rule-based parsing failed")

    # 3) æ­£è¦åŒ–ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    uni = str(data.get("university", "")).strip() or rule_based.get("university")
    year = str(data.get("year", "")).strip() or rule_based.get("year")
    subj = _normalize_subject(str(data.get("subject", "")).strip()) or rule_based.get("subject")
    q_num_raw = data.get("question_number")
    q_num = str(q_num_raw).strip() if q_num_raw is not None else None
    q_num = q_num or rule_based.get("question_number")
    if q_num == 'None' or not q_num:
        q_num = None

    if not uni or not year or not subj:
        # ãƒ«ãƒ¼ãƒ«æŠ½å‡ºãŒéƒ¨åˆ†çš„ã«ã§ã‚‚ã‚ã‚Œã°ãã‚Œã‚’è¿”ã™ï¼ˆå‡¦ç†ç¶™ç¶šã®ãŸã‚ï¼‰
        partial = {k: v for k, v in {"university": uni, "year": year, "subject": subj}.items() if v}
        if q_num:
            partial["question_number"] = q_num
        if partial:
            print(f"[extract_answer_details] è§£æä¸å®Œå…¨ã®ãŸã‚éƒ¨åˆ†çµæœã‚’è¿”å´: {partial}")
            return partial
        raise ValueError(f"Extracted details incomplete: university={uni}, year={year}, subject={subj}")

    # å¹´ã¯æ•°å­—ã®ã¿ã«æƒãˆã‚‹
    m = re.search(r"(20\d{2})", year)
    if not m:
        raise ValueError(f"Invalid year format: {year}")
    year = m.group(1)

    details = {"university": uni, "year": year, "subject": subj}
    if q_num:
        details["question_number"] = q_num
    print(f"[extract_answer_details] æœ€çµ‚ç¢ºå®š: {details}")
    return details



async def generate_review_comments(answer_markdown: str, problem_texts: List[str]) -> Dict:
    """
    æ·»å‰Šã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚å‡ºåŠ›ã¯å³å¯†ã«JSONã§ã€å„å°å•ã”ã¨ã«4å¤§å¿…é ˆè¦ç´ ï¼ˆâ‘ æ¡ç‚¹ã€â‘¡è³è³›ã€â‘¢èª¤ã‚Šã®æŒ‡æ‘˜ã€â‘£æ–¹é‡æç¤ºï¼‰ã‚’å«ã‚ã¾ã™ã€‚
    - answer_markdown: æ›¸ãèµ·ã“ã—æ¸ˆã¿ã®ç­”æ¡ˆMarkdown
    - problem_texts: å•é¡Œæ–‡ã‚„æ¡ç‚¹åŸºæº–ãªã©ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆè¤‡æ•°å¯ï¼‰
    è¿”å´ä¾‹:
    {
      "summary": {"total_score": 30, "max_score": 50, "notes": "..."},
      "questions": [
        {
          "id": "1",
          "awarded": 8,
          "max": 10,
          "comments": [
            {"type": "score", "text": "> ã€Œâ—‹â—‹ã€ã« +5ç‚¹", "points": 5, "target": "å¼(3è¡Œç›®)"},
            {"type": "praise", "text": "> âœ… ã€œãŒã§ãã¦ã„ã¦è‰¯ã„ã§ã™", "target": "æ–¹é‡"},
            {"type": "mistake", "text": "> âŒ ã€œã®ç®‡æ‰€ã§èª¤ã‚Šã§ã™ï¼ˆç†ç”±: â€¦ï¼‰", "target": "è¨ˆç®—(4è¡Œç›®)"},
            {"type": "guidance", "text": "> ğŸ’¡ æº€ç‚¹ã«è‡³ã‚‹ã«ã¯ã€œ", "target": "æ¬¡ã®æ‰‹é †"}
          ]
        }
      ]
    }
    """
    model = genai.GenerativeModel('gemini-2.5-pro')

    policy_text = textwrap.dedent("""
    æ·»å‰Šãƒ«ãƒ¼ãƒ«ï¼ˆå³å®ˆï¼‰:
    - åŸºæœ¬ç†å¿µ: ç”Ÿå¾’ã‚’è‚²ã¦ã‚‹ã€‚ä¸å¯§èªï¼ˆã§ã™ãƒ»ã¾ã™èª¿ï¼‰ã€‚å¦å®šçš„/æ”»æ’ƒçš„è¡¨ç¾ã¯é¿ã‘ã‚‹ã€‚
    - 4å¤§å¿…é ˆè¦ç´ ï¼ˆå„å°å•ã§å¿…ãšå«ã‚ã‚‹ï¼‰:
      â‘  æ­£èª¤åˆ¤å®šã¨æ¡ç‚¹: æ¡ç‚¹åŸºæº–ã«å³å¯†æº–æ‹ ã€‚åŠ ç‚¹æ³•ã€‚èª¤ã£ãŸéç¨‹ã®çµæœã¯åŠ ç‚¹ã—ãªã„ã€‚åˆ¥è§£ã¯æ­£ã—ã‘ã‚Œã°é…ç‚¹ç›¸å½“ã§åŠ ç‚¹ã€‚
      â‘¡ è³è³›: å…·ä½“çš„ã«è‰¯ã„ç‚¹ã‚’ã»ã‚ã‚‹ã€‚0ç‚¹ã§ã‚‚å¿…ãš1ã¤ã»ã‚ã‚‹ã€‚æº€ç‚¹ã§ã‚‚å…·ä½“çš„ã«ã»ã‚ã‚‹ã€‚
      â‘¢ èª¤ã‚Šã®æŒ‡æ‘˜: ã©ã“ãŒã€ãªãœèª¤ã‚Šã‹ã‚’å…·ä½“çš„ã«ã€‚ç­”æ¡ˆã®å†…å®¹ã«å³ã—ã¦æŒ‡æ‘˜ã€‚å˜ãªã‚‹æ¨¡ç¯„è§£ç­”ã®å†™ã—ã¯ä¸å¯ã€‚
      â‘£ æ–¹é‡æç¤º: ç”Ÿå¾’ã®æ–¹é‡ã‚’æ´»ã‹ã—ã¤ã¤æº€ç‚¹ã«è‡³ã‚‹ãŸã‚ã®ãƒ’ãƒ³ãƒˆã‚„åˆ¥è§£ã‚’ç¤ºã™ã€‚
    - å½¢å¼/è¨˜æ³•:
      * ã‚³ãƒ¡ãƒ³ãƒˆå¯¾è±¡ã‚’targetã«æ˜ç¢ºåŒ–ã€‚
      * ãƒ†ã‚­ã‚¹ãƒˆå†…ã«ã€Œ+nç‚¹ã€ã®è¡¨è¨˜ã¯æ›¸ã‹ãªã„ï¼ˆé…ç‚¹ã¯ points ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ã§è¡¨ã™ï¼‰ã€‚æ¸›ç‚¹ã¯ã—ãªã„ï¼ˆmistakeã§æŒ‡æ‘˜ï¼‰ã€‚
      * æ•°å¼ã¯**å¿…ãš**TeXå½¢å¼(ä¾‹ï¼š$\int_a^b f(x)\,\mathrm{d}x$)ã§å‡ºåŠ›ã€‚ãƒ‰ãƒ«ãƒãƒ¼ã‚¯2ã¤ãšã¤ã§å›²ã‚€è¨˜æ³•ã¯ç”¨ã„ãªã„ã€‚
      * å‡ºåŠ›ã¯JSONã®ã¿ã€‚questionã”ã¨ã« comments ã®é…åˆ—ã« type ã‚’ä»˜ä¸ï¼ˆscore/praise/mistake/guidanceï¼‰ã€‚
      * scoreã‚³ãƒ¡ãƒ³ãƒˆã¯â€œåŠ ç‚¹ç®‡æ‰€ã”ã¨â€ã«1è¦ç´ ä½œæˆã—ã€ãƒ†ã‚­ã‚¹ãƒˆã¯å…·ä½“çš„ãªè³è³›å†…å®¹ã®ã¿ã‚’æ›¸ãï¼ˆ+nç‚¹ã¯æ›¸ã‹ãªã„ï¼‰ã€‚targetã‚’æ˜è¨˜ã€‚
      * åŒä¸€å°å•å†…ã§ã€åŠ ç‚¹ãƒã‚¤ãƒ³ãƒˆã‚„èª¤ã‚ŠãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯ã€ãã‚Œãã‚Œåˆ¥ã®ã‚³ãƒ¡ãƒ³ãƒˆè¦ç´ ã¨ã—ã¦â€œã™ã¹ã¦â€åˆ—æŒ™ã™ã‚‹ï¼ˆ1ç‚¹ã«ã¤ã1è¦ç´ ï¼‰ã€‚
      * praiseã‚³ãƒ¡ãƒ³ãƒˆã¯ä»»æ„ã€‚å¿…è¦ãªã‚‰å°å•å…¨ä½“ã®ç·è©•ã¨ã—ã¦æœ€å¤§1ä»¶ã®ã¿ï¼ˆéå‰°ã«å¢—ã‚„ã•ãªã„ï¼‰ã€‚
      * awarded ã¯ scoreã‚³ãƒ¡ãƒ³ãƒˆã® points ã®åˆè¨ˆï¼ˆåŠ ç‚¹æ³•ï¼‰ã€‚max ã¯æ¡ç‚¹åŸºæº–ã«åˆã‚ã›ã€ä¸æ˜ãªå ´åˆã¯åˆç†çš„ã«ä»®ç½®ãã—ã¦ notes ã«æ ¹æ‹ ã‚’è¨˜è¼‰ã€‚
      * è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆã¯å¼•ç”¨è¨˜æ³•ï¼ˆ>ï¼‰ã¨çµµæ–‡å­—ï¼ˆâœ…, âŒ, ğŸ’¡ï¼‰ã‚’æ´»ç”¨ã€‚
    - ç‰¹æ®Šã‚±ãƒ¼ã‚¹:
      * å†æå‡ºã¯å‰å›ã¨ã®å·®åˆ†ã«è¨€åŠï¼ˆå‰å›æƒ…å ±ãŒç„¡ã„å ´åˆã¯çœç•¥å¯ï¼‰ã€‚
      * èª²ç¨‹å¤–ã®è§£æ³•: åŸå‰‡æº€ç‚¹ã€‚ä½†ã—èª²ç¨‹å†…ãªã‚‰é€šå¸¸ã©ãŠã‚Šæ¡ç‚¹ã€‚
      * ä¸å‚™ç­”æ¡ˆï¼ˆç™½ç´™/åˆ¤èª­ä¸èƒ½/å•é¡Œé•ã„ï¼‰ã¯ãã®æ—¨ã‚’notesã«è¨˜è¼‰ã—ã€æ¡ç‚¹ä¸èƒ½ã¨ã™ã‚‹ã€‚
    - æ¡ç‚¹åŸºæº–ãŒæ˜ç¤ºã•ã‚Œãªã„å ´åˆã¯ã€å•é¡Œæ–‡ãƒ»è§£ç­”ä¾‹ã‹ã‚‰åˆç†çš„ã«æ¨å®šã—ã¦é…ç‚¹ï¼ˆmaxï¼‰ã‚’ä»®ç½®ãã—ã€notesã«æ˜è¨˜ã€‚
    """)

    problems_joined = "\n\n".join(problem_texts[:6])  # é•·æ–‡å¯¾ç­–ã§æœ€å¤§æ•°ã‚’åˆ¶é™
    prompt = textwrap.dedent(f"""
    æ¬¡ã®ç­”æ¡ˆMarkdownã¨å•é¡Œ/æ¡ç‚¹åŸºæº–ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ãã€æ·»å‰Šã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
    å‡ºåŠ›ã¯å¿…ãšå³å¯†ãªJSONã®ã¿ã§ã€æŒ‡å®šã‚¹ã‚­ãƒ¼ãƒã«å¾“ã£ã¦ãã ã•ã„ã€‚

    {policy_text}

    [ç­”æ¡ˆMarkdown]
    ---
    {answer_markdown}
    ---

    [å•é¡Œãƒ»æ¡ç‚¹åŸºæº–ãƒ†ã‚­ã‚¹ãƒˆï¼ˆè¤‡æ•°ï¼‰]
    ---
    {problems_joined}
    ---

    æ³¨æ„:
    - å„ãƒ†ã‚­ã‚¹ãƒˆã®å…ˆé ­ã« [FILE:ãƒ•ã‚¡ã‚¤ãƒ«å] ãŒä»˜ã„ã¦ã„ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«åã«ã€Œæ¡ç‚¹ã€ã‚„ã€Œæ¡ç‚¹åŸºæº–ã€ã‚’å«ã‚€ã‚‚ã®ã¯é…ç‚¹æ ¹æ‹ ã¨ã—ã¦æœ€å„ªå…ˆã§å‚ç…§ã—ã¦ãã ã•ã„ã€‚
    - PDFç”±æ¥ã§ä¸€éƒ¨ã®ãƒ†ã‚­ã‚¹ãƒˆãŒæ¬ è½ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãã®å ´åˆã‚‚ã€åˆ©ç”¨å¯èƒ½ãªæƒ…å ±ã‹ã‚‰æœ€ã‚‚å¦¥å½“ãªé…ç‚¹ã¨æ¡ç‚¹æ ¹æ‹ ã‚’ summary.notes ã«æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚

    JSONã‚¹ã‚­ãƒ¼ãƒï¼ˆå³å¯†ï¼‰:
    {{
      "summary": {{"total_score": number, "max_score": number, "notes": string}},
      "questions": [
        {{
          "id": string,  // ä¾‹: "1", "2(1)" ãªã©
          "awarded": number,
          "max": number,
          "comments": [
            {{"type": "score"|"praise"|"mistake"|"guidance", "text": string, "target": string, "points": number|null}}
          ]
        }}
      ]
    }}
    """)

    def _try_parse_json(s: str) -> Optional[dict]:
        try:
            return json.loads(s)
        except Exception:
            return None

    def _fix_invalid_backslashes(s: str) -> str:
        # JSONã§ã¯ \ ã®å¾Œã«èªã‚ã‚‰ã‚Œã‚‹ã®ã¯ \ / " b f n r t u ã®ã¿ã€‚
        # ãã‚Œä»¥å¤–ï¼ˆä¾‹: \frac, \alpha ç­‰ï¼‰ã¯ç„¡åŠ¹ãªã®ã§ \\ ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã™ã‚‹ã€‚
        return re.sub(r"(?<!\\)\\(?![\\/\"bfnrtu])", r"\\\\", s)

    try:
        resp = await model.generate_content_async(prompt)
        raw = (getattr(resp, 'text', '') or '').strip()
        m = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", raw, re.IGNORECASE)
        if m:
            raw = m.group(1)
        data = _try_parse_json(raw)
        if data is None:
            fixed = _fix_invalid_backslashes(raw)
            data = _try_parse_json(fixed)
        if data is None:
            # ã•ã‚‰ã«ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å¤–ã‹ã‚‰æœ€åˆã® {..} ã‚’æ•‘å‡ºã—ã¦å†è©¦è¡Œ
            m2 = re.search(r"(\{[\s\S]*\})", raw)
            if m2:
                cand = _fix_invalid_backslashes(m2.group(1))
                data = _try_parse_json(cand)
        if data is None:
            raise ValueError("JSON parse failed after fixes")
        return data
    except Exception as e:
        # æœ€ä½é™ã®å½¢ã§è¿”ã™
        print(f"[generate_review_comments] ç”Ÿæˆå¤±æ•—: {e}")
        return {
            "summary": {"total_score": 0, "max_score": 0, "notes": f"ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"},
            "questions": []
        }


def normalize_review_cardinality(review: Dict) -> Dict:
    """
    ç¾æ–¹é‡ã§ã¯ã€scoreã‚³ãƒ¡ãƒ³ãƒˆè‡ªä½“ã«è³è³›æ–‡è¨€ã‚’å«ã‚ã‚‹ãŸã‚è‡ªå‹•è£œå®Œã¯è¡Œã‚ãªã„ã€‚
    å°†æ¥ã®æ¤œè¨¼/æ•´å½¢ã®ãŸã‚ã®ãƒ•ãƒƒã‚¯ã¨ã—ã¦ã€ãã®ã¾ã¾è¿”ã™ã€‚
    """
    return review


async def map_comments_to_blocks(blocks: List[Dict], comments: List[Dict]) -> List[Dict]:
    """
    Map review comments to page text blocks using Gemini. Returns a list of
    { index: int, block_id: string|null } in the same order as input comments.

    - blocks: [{ id, page, x0, y0, x1, y1, text }], where text is short (<=120 chars)
    - comments: [{ index, type, text, target }]
    """
    model = genai.GenerativeModel('gemini-2.5-pro')

    # Cap sizes to control token usage
    max_blocks = int(os.getenv('AUTO_LAYOUT_MAX_BLOCKS', '350'))
    max_text = int(os.getenv('AUTO_LAYOUT_BLOCK_TEXT_CAP', '120'))
    blocks_compact = []
    for b in blocks[:max_blocks]:
        t = (b.get('text') or '').strip()
        if len(t) > max_text:
            t = t[:max_text]
        blocks_compact.append({
            'id': b.get('id'), 'page': b.get('page'),
            'x0': b.get('x0'), 'y0': b.get('y0'), 'x1': b.get('x1'), 'y1': b.get('y1'),
            'text': t,
        })

    comments_compact = []
    for i, c in enumerate(comments):
        comments_compact.append({
            'index': int(c.get('index', i)),
            'type': c.get('type'),
            'text': (c.get('text') or '')[:160],
            'target': (c.get('target') or '')[:120],
            'points': c.get('points', None),
        })

    import json as _json
    prompt = (
        "æ¬¡ã®PDFãƒ†ã‚­ã‚¹ãƒˆãƒ–ãƒ­ãƒƒã‚¯ä¸€è¦§ã¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã®å¯¾å¿œä»˜ã‘ã‚’è¡Œã„ã¾ã™ã€‚\n"
        "å„ã‚³ãƒ¡ãƒ³ãƒˆã«æœ€ã‚‚é©åˆ‡ãªãƒ–ãƒ­ãƒƒã‚¯idã‚’1ã¤é¸ã³ã€è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°nullã«ã—ã¦ãã ã•ã„ã€‚\n"
        "åŸºæº–: targetãŒã‚ã‚Œã°æœ€å„ªå…ˆã§ä¸€è‡´ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã€ãªã‘ã‚Œã°æœ¬æ–‡ã®é‡è¦èªã§éƒ¨åˆ†ä¸€è‡´ã€‚é…ç‚¹(score)ã¯æœ¬æ–‡ã®è¿‘ãã§æ§‹ã„ã¾ã›ã‚“ã€‚\n"
        "å‡ºåŠ›ã¯JSONé…åˆ—ã®ã¿ã€‚å½¢å¼: [{\"index\": number, \"block_id\": string|null}]."
    )
    blocks_json = _json.dumps(blocks_compact, ensure_ascii=False)
    comments_json = _json.dumps(comments_compact, ensure_ascii=False)
    instr = f"{prompt}\n[blocks]\n{blocks_json}\n\n[comments]\n{comments_json}"

    try:
        resp = await model.generate_content_async(instr)
        text = (getattr(resp, 'text', '') or '').strip()
    except Exception as e:
        raise RuntimeError(f"gemini mapping failed: {e}")

    # Try parse JSON list
    import re
    m = re.search(r"```json\s*(\[[\s\S]*?\])\s*```", text, re.IGNORECASE)
    if m:
        text = m.group(1)
    try:
        data = _json.loads(text)
        if isinstance(data, list):
            return data
    except Exception:
        pass
    # Fallback empty mapping
    return [{ 'index': c['index'], 'block_id': None } for c in comments_compact]


async def spatial_locate(items: List[Dict], image_path: Path, *, normalize: bool = True) -> List[Dict]:
    """
    Use Gemini spatial understanding to locate target items on an image.
    items: [{ id: string|number, text: string }] - short, specific target text per item
    Returns: [{ id, x, y, w, h, confidence, reason }]
    Coordinates are normalized to 0..1 if normalize=True, else absolute pixels.
    """
    model = genai.GenerativeModel('gemini-2.5-pro')
    from PIL import Image
    img = Image.open(image_path)
    width, height = img.size

    # Compact items
    import json as _json
    items_compact = []
    for it in items:
        tid = str(it.get('id')) if 'id' in it else str(len(items_compact))
        txt = (it.get('text') or '').strip()
        if not txt:
            continue
        # limit length
        if len(txt) > 140:
            txt = txt[:140]
        items_compact.append({ 'id': tid, 'text': txt })
    if not items_compact:
        return []

    instruction = (
        "æ¬¡ã®ç”»åƒã®ä¸­ã‹ã‚‰ã€ä¸ãˆã‚‰ã‚ŒãŸå„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ†ã‚­ã‚¹ãƒˆã«æœ€ã‚‚å¯¾å¿œã™ã‚‹é ˜åŸŸã‚’1ã¤ãšã¤è¦‹ã¤ã‘ã€"
        "å„é ˜åŸŸã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ nullã€‚\n"
        "è¿”å´ã¯å³å¯†ãªJSONé…åˆ—ã®ã¿ã€‚å„è¦ç´ ã¯ {id, x, y, w, h, confidence, reason}ã€‚\n"
        f"ç”»åƒã‚µã‚¤ã‚º: width={width}, height={height}ã€‚"
    )
    if normalize:
        instruction += "åº§æ¨™ã¯0ã€œ1ã®æ­£è¦åŒ–ï¼ˆx,yã¯å·¦ä¸Šã€w,hã¯å¹…ã¨é«˜ã•ï¼‰ã€‚"
    else:
        instruction += "åº§æ¨™ã¯ãƒ”ã‚¯ã‚»ãƒ«ï¼ˆx,yã¯å·¦ä¸Šã®çµ¶å¯¾å€¤ã€w,hã¯å¹…ã¨é«˜ã•ï¼‰ã€‚"

    items_json = _json.dumps(items_compact, ensure_ascii=False)
    prompt = (
        instruction + "\n" +
        "[ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¸€è¦§]\n" + items_json + "\n" +
        "å‡ºåŠ›ã®ã¿ã‚’JSONã§: [{\"id\": string, \"x\": number|null, \"y\": number|null, \"w\": number|null, \"h\": number|null, \"confidence\": number|null, \"reason\": string}]."
    )

    resp = await model.generate_content_async([prompt, img])
    text = (getattr(resp, 'text', '') or '').strip()
    import re
    m = re.search(r"```json\s*(\[[\s\S]*?\])\s*```", text, re.IGNORECASE)
    if m:
        text = m.group(1)
    try:
        data = _json.loads(text)
    except Exception:
        return []
    if not isinstance(data, list):
        return []
    # sanitize
    out = []
    for d in data:
        try:
            tid = str(d.get('id'))
            x = d.get('x'); y = d.get('y'); w = d.get('w'); h = d.get('h')
            conf = d.get('confidence')
            reason = str(d.get('reason') or '')
            if x is None or y is None or w is None or h is None:
                out.append({ 'id': tid, 'x': None, 'y': None, 'w': None, 'h': None, 'confidence': conf, 'reason': reason })
            else:
                out.append({ 'id': tid, 'x': float(x), 'y': float(y), 'w': float(w), 'h': float(h), 'confidence': float(conf) if conf is not None else None, 'reason': reason })
        except Exception:
            continue
    return out

async def generate_curated_answer_md(
    sources: List[Dict[str, str]],
    *,
    university: str,
    exam_type: str,
    question: Optional[int] = None,
    title: Optional[str] = None,
) -> str:
    """
    Curate Answer.md using PDFs directly where possible, with text fallback.
    - sources: list of {name, kind('md'|'txt'|'pdf'), text?, path?}
    """
    model = genai.GenerativeModel('gemini-2.5-pro')

    # Prepare uploads/text
    uploaded_files: List = []
    text_blocks: List[str] = []
    per_item_cap = int(os.getenv('BUILD_MD_PER_ITEM_CAP', '16000'))
    total_text_cap = int(os.getenv('BUILD_MD_MAX_CONTEXT', '48000'))

    for it in sources:
        kind = (it.get('kind') or '').lower()
        name = it.get('name') or 'UNKNOWN'
        path = it.get('path')
        if kind == 'pdf' and path and Path(path).exists():
            try:
                f = genai.upload_file(path=path)
                uploaded_files.append((name, f))
                continue
            except Exception:
                # fall back to text below
                pass
        txt = (it.get('text') or '').strip()
        if txt:
            if len(txt) > per_item_cap:
                txt = txt[:per_item_cap]
            text_blocks.append(f"[FILE:{name}]\n{txt}")

    joined_text = "\n\n".join(text_blocks)[:total_text_cap]

    tgt_title = title or (f"{university} {exam_type} å‚ç…§ç”¨Markdown" if question is None else f"{university} {exam_type} å¤§å•{question} å‚ç…§ç”¨Markdown")

    policy = textwrap.dedent(
        """
        ç›®çš„: å•é¡Œã¨æ¡ç‚¹åŸºæº–ã‚’ä½™ã™ã“ã¨ãªãã€æ­£ç¢ºã«æ·»å‰Šã§ãã‚‹å‚ç…§ç”¨Markdownï¼ˆAnswer.mdï¼‰ã‚’ä½œã‚‹ã€‚
        å³å®ˆäº‹é …:
        - å‡ºåŠ›ã¯Markdownã®ã¿ã€‚YAML frontmatter + è¦‹å‡ºã—æ§‹é€ ã€‚
        - ä¸ãˆã‚‰ã‚ŒãŸã‚½ãƒ¼ã‚¹ä»¥å¤–ã‹ã‚‰ã®æ¨æ¸¬ãƒ»è£œå®Œã‚„æ–°è¦å†…å®¹ã®å‰µä½œã¯ä¸€åˆ‡ç¦æ­¢ï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å³ç¦ï¼‰ã€‚
        - æƒ…å ±æ¬ è½ãŒã‚ã‚‹ç®‡æ‰€ã¯ NOTE ã‚’æ˜ç¤ºã—ã€ç©ºæ¬„ã‚’ç„¡ç†ã«åŸ‹ã‚ãªã„ã€‚
        - æ•°å¼ã¯TeXï¼ˆã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã¯ $...$ã€ãƒ–ãƒ­ãƒƒã‚¯ã¯ $$...$$ï¼‰ã€‚PDFç”±æ¥ã®å´©ã‚Œã¯æ¥µåŠ›æ•´å½¢ã€‚
        - å¤§å•/å°å•ã®æ§‹é€ ã‚’æ˜ç¢ºã«ã€‚
        - é‡è¦(æ§‹æˆã®ç¦æ­¢äº‹é …): å…¨å°å•ã®ã€Œå•é¡Œã€ã‚’å…ˆã«ã¾ã¨ã‚ã¦ç¾…åˆ—ã—ãŸã‚Šã€ç« æœ«ã«ã€Œæ¡ç‚¹åŸºæº–ã€ã‚’ä¸€æ‹¬ã§ã¾ã¨ã‚ãªã„ã“ã¨ã€‚å„å°å•å†…ã§å®Œçµã•ã›ã‚‹ã€‚
        - é‡è¦(åŠ ç‚¹ã®è¡¨è¨˜): æ¨¡ç¯„è§£ç­”ã®ã©ã®éƒ¨åˆ†ã«å¯¾ã™ã‚‹è©•ä¾¡ãªã®ã‹ãŒæ˜ç¢ºã«åˆ†ã‹ã‚‹ã‚ˆã†ã«ã€> ã€æ¡ç‚¹ã€‘... [+Xç‚¹] ã®å½¢å¼ã§ã€å¯¾å¿œã™ã‚‹è§£ç­”ã®ç›´å¾Œã«è¨˜è¿°ã™ã‚‹ã€‚
          ã•ã‚‰ã«å°å•æœ«å°¾ã«ã€Œé…ç‚¹ã¾ã¨ã‚: +2 +3 +5 = 10ã€ã®ã‚ˆã†ã«åˆè¨ˆã‚’ç¤ºã™ã€‚åŠ ç‚¹ç†ç”±ã‚‚ã¨ã‚‚ã«ç¤ºã™ã€‚æ ¹æ‹ ãŒä¸æ˜ãªã‚‰ NOTE ã‚’ä»˜ã™ã€‚
        - é…ç‚¹ã®æ ¹æ‹ ã¯ã€ã‚½ãƒ¼ã‚¹å†…ã«ã€Œæ¡ç‚¹ã€ã€Œæ¡ç‚¹åŸºæº–ã€ç­‰ã®æ–‡æ›¸ãŒã‚ã‚Œã°æœ€å„ªå…ˆã§åæ˜ ã€‚
        - ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆ[FILE:...]ï¼‰ãŒã‚ã‚Œã°ä»˜è¨˜ã—ã¦å‡ºå…¸é–¢ä¿‚ã‚’ä¿ã¤ã€‚
        - ç”¨èªã‚„è¨˜å·ã¯åŸæ–‡ã«å¿ å®Ÿã€‚æ›–æ˜§ã•ã¯ NOTE ã§æ³¨è¨˜ã€‚
        æ›¸å¼:
        - Frontmatter: title/version/source ã‚’è¨˜è¼‰ã€‚
        - ç« ç«‹ã¦: # ã‚¿ã‚¤ãƒˆãƒ« -> å°å•ã”ã¨ã« ### (1), ### (2), ... ã¨ç¶šã‘ã‚‹ï¼ˆå„å°å•å†…ã§ã€å•é¡Œã€‘ã€è§£ç­”ã€‘ã€é…ç‚¹ã¾ã¨ã‚ã€‘ã‚’å®Œçµï¼‰ã€‚
        - å¿…è¦ã«å¿œã˜ã¦ã€Œå‚è€ƒã€ç¯€ã‚’è¨­ã‘ã¦ã‚‚ã‚ˆã„ï¼ˆå‡ºå…¸ã®æ–­ç‰‡ã‚„è£œè¶³ã‚’ç®‡æ¡æ›¸ãï¼‰ã€‚
        å‡ºåŠ›ã¯ã€æ—¥æœ¬èªã§ç°¡æ½”ãƒ»æ­£ç¢ºãƒ»å†åˆ©ç”¨ã—ã‚„ã™ã„å½¢ã«ã™ã‚‹ã€‚
        """
    )

    meta = textwrap.dedent(
        f"""
        [ãƒ¡ã‚¿æƒ…å ±]
        - å¤§å­¦: {university}
        - è©¦é¨“ç¨®: {exam_type}
        - å¤§å•: {question if question is not None else '(å…¨ä½“)'}
        - å‡ºåŠ›ã‚¿ã‚¤ãƒˆãƒ«: {tgt_title}
        """
    )

    instructions = textwrap.dedent(
        f"""
        ã‚ãªãŸã¯å³å¯†ãªã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç·¨é›†è€…ã§ã™ã€‚ä»¥ä¸‹ã®ã‚½ãƒ¼ã‚¹ç¾¤ï¼ˆæ·»ä»˜PDFã¨ãƒ†ã‚­ã‚¹ãƒˆæ–­ç‰‡ï¼‰ã®ã¿ã‚’æ ¹æ‹ ã¨ã—ã¦ã€
        æ¡ç‚¹ã«ä½¿ãˆã‚‹å‚ç…§ç”¨Markdownã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚å¤–éƒ¨çŸ¥è­˜ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚

        {meta}

        {policy}

        æ·»ä»˜PDFã¯å„ªå…ˆçš„ã«å‚ç…§ã—ã€ãƒ†ã‚­ã‚¹ãƒˆæ–­ç‰‡ã¯è£œåŠ©ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„ã€‚

        æŒ‡ç¤º:
        1) å°å•(1),(2),...æ¯ã«ç‹¬ç«‹ã—ãŸæ§‹æˆã‚’ä½œã‚‹ï¼ˆã€å•é¡Œã€‘â†’ã€è§£ç­”ã€‘â†’é…ç‚¹ã¾ã¨ã‚ï¼‰ã€‚å…¨å°å•ã®å•é¡Œã ã‘ã‚’å…ˆã«ç¾…åˆ—ã—ãŸã‚Šã€ç« æœ«ã«æ¡ç‚¹ã‚’é›†ç´„ã—ãªã„ã“ã¨ã€‚
        2) å•é¡Œæ–‡ã‚’å¾©å…ƒï¼ˆèª¤æ¤ä¿®æ­£å¯ï¼‰ã€‚å¿…è¦ã«å¿œã˜ã¦è¦ç´„ãƒ»å¼•ç”¨ã‚’ä½µç”¨ã€‚
        3) æ¨¡ç¯„è§£ç­”ã®ã©ã®éƒ¨åˆ†ã«å¯¾ã™ã‚‹è©•ä¾¡ãªã®ã‹ãŒæ˜ç¢ºã«åˆ†ã‹ã‚‹ã‚ˆã†ã«ã€> ã€æ¡ç‚¹ã€‘... [+Xç‚¹] ã®å½¢å¼ã§ã€å¯¾å¿œã™ã‚‹è§£ç­”ã®ç›´å¾Œã«è¨˜è¿°ã™ã‚‹ã€‚
        4) å°å•æœ«å°¾ã«ã€Œé…ç‚¹ã¾ã¨ã‚: +2 +3 +5 = 10ã€ã‚’è¨˜è¼‰ã€‚åŠ ç‚¹ç†ç”±ã‚‚ç¤ºã™ã“ã¨ã€‚æ ¹æ‹ ãŒä¸æ˜ãªé…ç‚¹ã¯ NOTE ã‚’ä»˜ã™ã€‚
        5) æ•°å¼ã¯æ­£ã—ã„TeXã¸æ•´å½¢ã€‚
        6) æœ€çµ‚å‡ºåŠ›ã¯ã€frontmatter + è¦‹å‡ºã—æ§‹é€ ã®Markdownæœ¬æ–‡ã®ã¿ã€‚
        """
    )

    inputs: List = [instructions]
    for name, f in uploaded_files:
        inputs.append(f)
    if joined_text:
        inputs.append(textwrap.dedent(f"[ãƒ†ã‚­ã‚¹ãƒˆæ–­ç‰‡]\n---\n{joined_text}\n---"))

    if len(inputs) == 1:
        return "# å‚ç…§Markdown\n\nï¼ˆå…¥åŠ›ã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰\n"

    try:
        resp = await model.generate_content_async(inputs)
        text = (getattr(resp, 'text', '') or '').strip()
    except Exception:
        # Fallback to text-only prompt
        fallback = textwrap.dedent(
            f"""
            {meta}

            {policy}

            [ã‚½ãƒ¼ã‚¹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆæ–­ç‰‡ã®ã¿ï¼‰]
            ---
            {joined_text}
            ---

            å‡ºåŠ›ã¯Markdownæœ¬æ–‡ã®ã¿ã€‚
            """
        )
        resp = await model.generate_content_async(fallback)
        text = (getattr(resp, 'text', '') or '').strip()

    m = re.search(r"```(?:md|markdown)?\s*([\s\S]*?)\s*```", text, re.IGNORECASE)
    if m:
        return m.group(1).strip()
    return text or ""
